import onnxruntime as ort
import numpy as np
from .preprocessing import preprocess_image, preprocess_audio
from PIL import Image
from typing import Dict, Any

# -------------------------------
# Model Loader
# -------------------------------
def load_model(model_path: str) -> ort.InferenceSession:
    """
    Load ONNX model using ONNX Runtime
    """
    print(f"ðŸ”§ Loading model from: {model_path}")
    return ort.InferenceSession(model_path)

# -------------------------------
# Text / GPT Inference
# -------------------------------
def run_inference(model: ort.InferenceSession, prompt: str, max_tokens=128) -> str:
    """
    Run text inference using ONNX model outputs where available,
    with deterministic fallback summarization if model IO does not match.
    """
    text = (prompt or "").strip()
    if not text:
        return "Please provide input text."

    encoded = np.frombuffer(text.encode("utf-8"), dtype=np.uint8).astype(np.int64)
    if encoded.size == 0:
        encoded = np.array([0], dtype=np.int64)
    encoded = encoded[:max_tokens]

    input_names = [x.name for x in model.get_inputs()]
    output_names = [x.name for x in model.get_outputs()]

    feeds: Dict[str, Any] = {}
    if "input_ids" in input_names:
        feeds["input_ids"] = encoded.reshape(1, -1)
    elif input_names:
        feeds[input_names[0]] = encoded.reshape(1, -1)

    try:
        outputs = model.run(output_names, feeds)
        if outputs:
            first = np.array(outputs[0])
            if first.size > 0:
                if first.ndim >= 2:
                    token_ids = np.argmax(first, axis=-1).flatten()
                else:
                    token_ids = np.clip(first.flatten().astype(np.int64), 0, 255)
                decoded = bytes([int(t) % 256 for t in token_ids[:max_tokens]]).decode("utf-8", errors="ignore").strip()
                if decoded:
                    return decoded
    except Exception:
        pass

    # Deterministic fallback: concise actionable response derived from prompt.
    words = text.split()
    head = " ".join(words[: min(len(words), 24)])
    if len(words) > 24:
        head += " ..."
    return f"NeuroEdge summary: {head}"

# -------------------------------
# Image Inference
# -------------------------------
def run_image_inference(model: ort.InferenceSession, img: Image.Image) -> dict:
    """
    Run image recognition inference.
    """
    input_data = preprocess_image(img)
    input_names = [x.name for x in model.get_inputs()]
    output_names = [x.name for x in model.get_outputs()]
    feeds: Dict[str, Any] = {}
    if input_names:
        feeds[input_names[0]] = input_data
    try:
        outputs = model.run(output_names, feeds)
        logits = np.array(outputs[0]).flatten() if outputs else np.array([])
        if logits.size > 0:
            idx = int(np.argmax(logits))
            conf = float(np.max(logits))
            return {"label": f"class_{idx}", "confidence": round(conf, 6)}
    except Exception:
        pass
    return {"label": "unknown", "confidence": 0.0}

# -------------------------------
# Audio Inference
# -------------------------------
def run_audio_inference(model: ort.InferenceSession, audio_array: np.ndarray) -> str:
    """
    Run audio transcription inference.
    """
    features = preprocess_audio(audio_array)
    input_names = [x.name for x in model.get_inputs()]
    output_names = [x.name for x in model.get_outputs()]
    feeds: Dict[str, Any] = {}
    if input_names:
        feeds[input_names[0]] = features
    try:
        outputs = model.run(output_names, feeds)
        if outputs:
            arr = np.array(outputs[0]).flatten()
            if arr.size > 0:
                clipped = np.clip(arr.astype(np.int64), 0, 255)[:256]
                text = bytes([int(v) for v in clipped]).decode("utf-8", errors="ignore").strip()
                if text:
                    return text
    except Exception:
        pass
    return "Audio processed; no transcription tokens produced."
